{"tokenizer": {"model_name": "gpt2-large", "padding_side": "left", "truncation_side": "left", "pad_token_as_eos_token": true}, "reward_fn": {"id": "pictionary_reward", "args": {"done_only": false}}, "datapool": {"id": "pictionary_datapool"}, "env": {"n_envs": 10, "args": {"max_prompt_length": 128, "max_episode_length": 24, "terminate_on_eos": true}}, "alg": {"id": "ppo", "args": {"n_steps": 128, "batch_size": 16, "verbose": 1, "learning_rate": 1e-06, "n_epochs": 5}, "kl_div": {"coeff": 0.2, "target_kl": 0.5}, "policy": {"id": "causal_lm_actor_critic_policy", "args": {"model_name": "gpt2-large", "apply_model_parallel": true, "generation_kwargs": {"do_sample": true, "top_k": 100, "min_length": 2, "max_new_tokens": 24}}}}, "train_evaluation": {"eval_batch_size": 16, "n_iters": 50, "eval_every": 5, "save_every": 10, "metrics": [{"id": "pictionary_metric"}, {"id": "causal_perplexity", "args": {"tokenizer_id": "gpt2-large", "stride": 128, "model_type": "causal"}}], "generation_kwargs": {"do_sample": true, "top_k": 100, "min_length": 2, "max_new_tokens": 24}}}